training:
  batch_size: 2
  grad_accum_steps: 8
  learning_rate: 1.0e-4
  weight_decay: 5.0e-4
  num_epochs: 20
  device: "cuda"
  num_workers: 2
  backbone_lr_mult: 0.1
  freeze_backbone_epochs: 5
  label_smoothing: 0.1
  focal_gamma: 0.0
  dice_weight: 0.0
  class_weights: null
  early_stopping:
    patience: 10
    min_delta: 0.0
  optimizer:
    type: "sgd"
    momentum: 0.9
    nesterov: false
  scheduler:
    type: "none"
    
  logging:
    use_wandb: false
    log_interval: 10
    save_dir: "experiments/checkpoints"
