training:
  batch_size: 2
  learning_rate: 5.0e-5
  weight_decay: 5.0e-4
  num_epochs: 60
  device: "cuda"
  num_workers: 2
  backbone_lr_mult: 0.05
  freeze_backbone_epochs: 0
  label_smoothing: 0.0
  focal_gamma: 2.0
  class_weights: [0.5, 1.0, 1.0, 1.2, 3.0, 1.5, 1.0]
  early_stopping:
    patience: 5
    patience: 10
    min_delta: 0.0
  optimizer:
    type: "adamw"
    
  scheduler:
    type: "cosine_annealing"
    T_max: 40
    T_max: 60
    
  logging:
    use_wandb: false
    log_interval: 10
    save_dir: "experiments/checkpoints"
