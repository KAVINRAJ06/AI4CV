model:
  vision_encoder:
    type: "vit_large_patch16_224"
    pretrained: true
    freeze_backbone: false
    unfreeze_last_n_blocks: 0
    drop_rate: 0.0
    drop_path_rate: 0.3
  
  text_encoder:
    type: "clip"
    model_name: "openai/clip-vit-base-patch16"
    freeze_text_encoder: true

  prompt_encoder:
    dropout: 0.1
    
  decoder:
    type: "unet"
    num_classes: 7
    hidden_dim: 768
    dropout: 0.2
    unet_width: 512
    unet_depth: 5
    use_skips: true
    
  cross_attention:
    num_heads: 12
    dropout: 0.2

hyperparameters:
  image_size: 512
  max_seq_len: 77
