model:
  vision_encoder:
    type: "vit_base_patch16_224" # or dinov2_vits14
    pretrained: true
    freeze_backbone: false
    unfreeze_last_n_blocks: 0
    drop_rate: 0.0
    drop_path_rate: 0.2
  
  text_encoder:
    type: "clip"
    model_name: "openai/clip-vit-base-patch16"
    freeze_text_encoder: true

  prompt_encoder:
    dropout: 0.1
    
  decoder:
    type: "unet"
    num_classes: 7
    hidden_dim: 512
    dropout: 0.3
    unet_width: 256
    unet_depth: 4
    
  cross_attention:
    num_heads: 8
    dropout: 0.2

hyperparameters:
  image_size: 512
  max_seq_len: 77
